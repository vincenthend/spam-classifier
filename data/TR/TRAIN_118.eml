{'body': 'Disclaimer:  I\'m partial to XFS\r\n\r\nTim Clewlow put forth on 5/1/2010 2:44 AM:\r\n\r\n> My reticence to use ext4 / xfs has been due to long cache before\r\n> write times being claimed as dangerous in the event of kernel lockup\r\n> / power outage. \r\n\r\nThis is a problem with the Linux buffer cache implementation, not any one\r\nfilesystem.  The problem isn\'t the code itself, but the fact it is a trade\r\noff between performance and data integrity.  No journaling filesystem will\r\nprevent the loss of data in the Linux buffer cache when the machine crashes.\r\n What they will do is zero out or delete any files that were not fully\r\nwritten before the crash in order to keep the FS in a consistent state.  You\r\nwill always lose data that\'s in flight, but your FS won\'t get corrupted due\r\nto the journal replay after reboot.  If you are seriously concerned about\r\nloss of write data that is in the buffer cache when the system crashes, you\r\nshould mount your filesystems with "-o sync" in the fstab options so all\r\nwrites get flushed to disk without being queued in the buffer cache.\r\n\r\n> There are also reports (albeit perhaps somewhat\r\n> dated) that ext4/xfs still have a few small but important bugs to be\r\n> ironed out - I\'d be very happy to hear if people have experience\r\n> demonstrating this is no longer true. My preference would be ext4\r\n> instead of xfs as I believe (just my opinion) this is most likely to\r\n> become the successor to ext3 in the future.\r\n\r\nI can\'t speak well to EXT4, but XFS has been fully production quality for\r\nmany years, since 1993 on Irix when it was introduced, and since ~2001 on\r\nLinux.  There was a bug identified that resulted in fs inconsistency after a\r\ncrash which was fixed in 2007.  All bug fix work since has dealt with minor\r\nissues unrelated to data integrity.  Most of the code fix work for quite\r\nsome time now has been cleanup work, optimizations, and writing better\r\ndocumentation.  Reading the posts to the XFS mailing list is very\r\ninformative as to the quality and performance of the code.  XFS has some\r\nreally sharp devs.  Most are current or former SGI engineers.\r\n\r\n> I have been wanting to know if ext3 can handle >16TB fs.  I now know\r\n> that delayed allocation / writes can be turned off in ext4 (among\r\n> other tuning options I\'m looking at), and with ext4, fs sizes are no\r\n> longer a question. So I\'m really hoping that ext4 is the way I can\r\n> go.\r\n\r\nXFS has even more tuning options than EXT4--pretty much every FS for that\r\nmatter.  With XFS on a 32 bit kernel the max FS and file size is 16TB.  On a\r\n64 bit kernel it is 9 exabytes each.  XFS is a better solution than EXT4 at\r\nthis point.  Ted T\'so admits last week that one function call in EXT4 is in\r\nterrible shape and will a lot of work to fix:\r\n\r\n"On my todo list is to fix ext4 to not call write_cache_pages() at all.\r\nWe are seriously abusing that function ATM, since we\'re not actually\r\nwriting the pages when we call write_cache_pages().  I won\'t go into\r\nwhat we\'re doing, because it\'s too embarassing, but suffice it to say\r\nthat we end up calling pagevec_lookup() or pagevec_lookup_tag()\r\n*four*, count them *four* times while trying to do writeback.\r\n\r\nI have a simple patch that gives ext4 our own copy of\r\nwrite_cache_pages(), and then simplifies it a lot, and fixes a bunch\r\nof problems, but then I discarded it in favor of fundamentally redoing\r\nhow we do writeback at all, but it\'s going to take a while to get\r\nthings completely right.  But I am working to try to fix this."\r\n\r\n> I\'m also hoping that a cpu/motherboard with suitable grunt and fsb\r\n> bandwidth could reduce performance problems with software raid6. If\r\n> I\'m seriously mistaken then I\'d love to know beforehand. My\r\n> reticence to use hw raid is that it seems like adding one more point\r\n> of possible failure, but I could be easily be paranoid in dismissing\r\n> it for that reason.\r\n\r\nGood hardware RAID cards are really nice and give you some features you\r\ncan\'t really get with md raid such as true "just yank the drive tray out"\r\nhot swap capability.  I\'ve not tried it, but I\'ve read that md raid doesn\'t\r\nlike it when you just yank an active drive.  Fault LED drive, audible\r\nwarnings, are also nice with HW RAID solutions.  The other main advantage is\r\nperformance.  Decent HW RAID is almost always faster than md raid, sometimes\r\nby a factor of 5 or more depending on the disk count and RAID level.\r\nTypically good HW RAID really trounces md raid performance at levels such as\r\n5, 6, 50, 60, basically anything requiring parity calculations.\r\n\r\nSounds like you\'re more of a casual user who needs lots of protected disk\r\nspace but not necessarily absolute blazing speed.  Linux RAID should be fine.\r\n\r\nTake a closer look at XFS before making your decision on a FS for this\r\narray.  It\'s got a whole lot to like, and it has features to exactly tune\r\nXFS to your mdadm RAID setup.  In fact it\'s usually automatically done for\r\nyou as mkfs.xfs queries the block device device driver for stride and width\r\ninfo, then matches it.  (~$ man 8 mkfs.xfs)\r\n\r\nhttp://oss.sgi.com/projects/xfs/\r\nhttp://www.xfs.org/index.php/XFS_FAQ\r\nhttp://www.debian-administration.org/articles/388\r\nhttp://www.jejik.com/articles/2008/04/benchmarking_linux_filesystems_on_software_raid_1/\r\nhttp://www.osnews.com/story/69\r\n(note the date, and note the praise Hans Reiser lavishes upon XFS)\r\nhttp://everything2.com/index.pl?node_id=1479435\r\nhttp://erikugel.wordpress.com/2010/04/11/setting-up-linux-with-raid-faster-slackware-with-mdadm-and-xfs/\r\nhttp://btrfs.boxacle.net/repository/raid/2010-04-14_2004/2.6.34-rc3/2.6.34-rc3.html\r\n(2.6.34-rc3 benchmarks, all filesystems in tree)\r\n\r\nXFS Users:\r\n\r\n The Linux Kernel Archives\r\n\r\n"A bit more than a year ago (as of October 2008) kernel.org, in an ever\r\nincreasing need to squeeze more performance out of it\'s machines, made the\r\nleap of migrating the primary mirror machines (mirrors.kernel.org) to XFS.\r\nWe site a number of reasons including fscking 5.5T of disk is long and\r\npainful, we were hitting various cache issues, and we were seeking better\r\nperformance out of our file system."\r\n\r\n"After initial tests looked positive we made the jump, and have been quite\r\nhappy with the results. With an instant increase in performance and\r\nthroughput, as well as the worst xfs_check we\'ve ever seen taking 10\r\nminutes, we were quite happy. Subsequently we\'ve moved all primary mirroring\r\nfile-systems to XFS, including www.kernel.org , and mirrors.kernel.org. With\r\nan average constant movement of about 400mbps around the world, and with\r\npeaks into the 3.1gbps range serving thousands of users simultaneously it\'s\r\nbeen a file system that has taken the brunt we can throw at it and held up\r\nspectacularly."\r\n\r\n-- \r\nStan\r\n\r\n\r\n-- \r\nTo UNSUBSCRIBE, email to debian-user-REQUEST@lists.debian.org \r\nwith a subject of "unsubscribe". Trouble? Contact listmaster@lists.debian.org\r\nArchive: http://lists.debian.org/4BDD5B56.8060705@hardwarefreak.com\r\n\r\n\r\n', 'subject': 'Re: Questions about RAID 6'}